{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function\n",
    "\n",
    "def scraper(keyword, start_page = 1, end_page = 100):\n",
    "    \n",
    "    #Timer - Start\n",
    "    start_time_outer = datetime.now()\n",
    "    \n",
    "    # Login \n",
    "    driver = webdriver.Chrome('/Users/sharm/webdrivers/chromedriver') #Path to Chrome driver exe\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    username = driver.find_element_by_id('username')\n",
    "    username.send_keys('email@gmail.com') #Enter Email gere\n",
    "\n",
    "    password = driver.find_element_by_id('password')\n",
    "    password.send_keys('P@ssw0rd')  #enter password here\n",
    "\n",
    "    login_button =  driver.find_element_by_class_name('btn__primary--large')\n",
    "    login_button.click()\n",
    "    search = keyword.replace(' ','%20') #Search keyword\n",
    "    sleep(2)\n",
    "    \n",
    "    # Keyword search\n",
    "    driver.get('https://www.linkedin.com/search/results/people/?keywords='+ search +'&origin=GLOBAL_SEARCH_HEADER&')\n",
    "    driver.execute_script(\"window.scrollTo(0,1000);\")\n",
    "    sleep(2)\n",
    "    pages = driver.find_elements_by_class_name('artdeco-pagination__indicator')\n",
    "    \n",
    "        \n",
    "    # Pagination\n",
    "    last_page = int(pages[-1].text)\n",
    "    \n",
    "    if end_page >= last_page:\n",
    "        end_page = last_page\n",
    "\n",
    "    page_range = range(start_page, end_page+1)\n",
    "    sleep(1)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for page_count in page_range:\n",
    "        page_link = 'https://www.linkedin.com/search/results/people/?keywords='+ search +'&origin=GLOBAL_SEARCH_HEADER&page='+str(page_count)\n",
    "        driver.get(page_link)\n",
    "        sleep(1)\n",
    "\n",
    "        # Collecting profile links\n",
    "        driver.execute_script(\"window.scrollTo(0,1000);\")\n",
    "        sleep(2)\n",
    "\n",
    "        # List of results\n",
    "        result= driver.find_elements_by_class_name('search-result__result-link')\n",
    "\n",
    "        for i in range(0,len(result),2):\n",
    "            urls.append(result[i].get_attribute('href'))\n",
    "        sleep(0.5)\n",
    "\n",
    "    # Accessing csv file\n",
    "\n",
    "    filename = 'candidates.csv'  \n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        append_write = 'a'\n",
    "    \n",
    "    else:\n",
    "        append_write = 'w'\n",
    "        \n",
    "        \n",
    "    f = open(filename, append_write)\n",
    "    \n",
    "    if append_write == 'w':\n",
    "        headers = 'Name,Title,Designation,Company,Duration,College,Degree,Subject,Skills,URL,Searched_By\\n'\n",
    "        f.write(headers)\n",
    "\n",
    "\n",
    "    counter = 1 # To keep track of profiles\n",
    "    \n",
    "\n",
    "    # Scrapping\n",
    "\n",
    "    for profile_link in urls:\n",
    "\n",
    "        driver.get(profile_link)\n",
    "        \n",
    "        try:\n",
    "            start_time_inner = datetime.now()   # Start time for profile extraction\n",
    "            ## Profile extraction\n",
    "\n",
    "            # Heading\n",
    "            try:\n",
    "                name = driver.find_element_by_class_name('inline').text.replace(',','-')\n",
    "                title = driver.find_elements_by_class_name('mt1')[1].text.replace(',','-')\n",
    "                driver.execute_script(\"window.scrollTo(0,500);\")\n",
    "                # sleep(1)\n",
    "            except NoSuchElementException:\n",
    "                name = 'N/A'\n",
    "                title = 'N/A'\n",
    "\n",
    "            # Experience\n",
    "            try:\n",
    "\n",
    "                WebDriverWait(driver, 10).until(lambda x: x.find_element_by_class_name('pv-entity__position-group-pager'))\n",
    "                sleep(1)\n",
    "                exp = driver.find_elements_by_class_name('pv-entity__position-group-pager')\n",
    "\n",
    "                designation = []\n",
    "                company = []\n",
    "                duration = []\n",
    "\n",
    "                for idx in range(len(exp)):\n",
    "                    exp_details = exp[idx].text.split('\\n')[::2]\n",
    "                    designation.append(exp_details[0].replace(',','-'))\n",
    "                    company.append(exp_details[1].replace(',','-'))\n",
    "\n",
    "                    if len(exp_details)>=4:\n",
    "                        duration.append(exp_details[3])\n",
    "                    #sleep(0.5)\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                designation = 'N/A'\n",
    "                company = 'N/A'\n",
    "                duration = 'N/A'\n",
    "\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Education\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(lambda x: x.find_elements_by_id('education-section'))\n",
    "                sleep(1)\n",
    "                edu = driver.find_elements_by_id('education-section')\n",
    "                college = []\n",
    "                degree = []\n",
    "                subject = []\n",
    "\n",
    "                for idx in range(len(edu)):\n",
    "                    edu_details = edu[0].text.split('\\n')\n",
    "                    college.append(edu_details[1].replace(',','-'))\n",
    "\n",
    "                    if len(edu_details)>=4:\n",
    "                        degree.append(edu_details[3].replace(',','-'))\n",
    "\n",
    "                    if len(edu_details)>=6:\n",
    "                        subject.append(edu_details[5].replace(',','-'))\n",
    "                sleep(0.5)\n",
    "\n",
    "            except NoSuchElementException:            \n",
    "                college = 'N/A'\n",
    "                degree = 'N/A'\n",
    "                subject = 'N/A'\n",
    "\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Skills\n",
    "            ## Srolling down to extract skills\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0,2000);\")\n",
    "            sleep(3)\n",
    "\n",
    "            try:\n",
    "                skill_more_button = driver.find_elements_by_class_name('pv-profile-section__card-action-bar')[0]\n",
    "                #skill_more_button.click()\n",
    "                driver.execute_script(\"arguments[0].click();\", skill_more_button)\n",
    "\n",
    "                sleep(1)\n",
    "\n",
    "\n",
    "                skill_class = driver.find_elements_by_class_name('pv-skill-category-entity__name')\n",
    "                skills = ' | '.join([skill.text.replace(',','-') for skill in skill_class])\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                skills = 'N/A'\n",
    "\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            # Formatting\n",
    "            designation_info = ' | '.join(designation)\n",
    "            company_info = ' | '.join(company)\n",
    "            duration_info = ' | '.join(duration) \n",
    "            college_info = ' | '.join(college)\n",
    "            degree_info = ' | '.join(degree)\n",
    "            subject_info = ' | '.join(subject)\n",
    "            profile_url = profile_link\n",
    "\n",
    "            try:\n",
    "                f.write(name + ',' +title+ ',' + designation_info + ',' + company_info + ',' + duration_info + ',' + \n",
    "                        college_info + ',' + degree_info + ',' + subject_info + ',' + skills + ',' + \n",
    "                        profile_url + ',' + keyword + '\\n')\n",
    "                \n",
    "                end_time_inner = datetime.now() \n",
    "                time_diff_inner = end_time_inner - start_time_inner\n",
    "                \n",
    "                print('Profiles extracted:' + str(counter) + '  - time taken: '\n",
    "                      +str(time_diff_inner.seconds)+'.'+str(time_diff_inner.microseconds)[:3]+' seconds')\n",
    "                counter += 1\n",
    "\n",
    "            except UnicodeEncodeError:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Saving the file\n",
    "    driver.close()\n",
    "    f.close()\n",
    "    \n",
    "    #Timer - Outer\n",
    "    end_time_outer = datetime.now()\n",
    "    time_diff_outer = end_time_outer - start_time_outer\n",
    "    \n",
    "    print('Program Execution Time: '+ str(time_diff_outer)[:-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function\n",
    "# Here I will be scraping details of profiles with Python tag from 1st to 10th page\n",
    "\n",
    "scraper('python', 1, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the data first 10 and last 10 entries\n",
    "df = pd.read_csv('test.csv', engine= 'python')\n",
    "\n",
    "pd.concat([df.head(10),df.tail(10)])\n",
    "\n",
    "##### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
